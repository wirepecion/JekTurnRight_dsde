{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Data Science Notebook\n",
    "\n",
    "This notebook demonstrates basic data analysis workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Add src to path\n",
    "# sys.path.insert(0, str(Path().resolve().parent / 'src'))\n",
    "\n",
    "sys.path.append(str(ROOT / \"src\"))\n",
    "os.environ[\"SPARK_LOCAL_DIRS\"] = str((ROOT / \"spark_tmp\").resolve())\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "from ds.analyzer import DataAnalyzer\n",
    "from de.load.loader import DataLoader\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'feature1': np.random.randn(100),\n",
    "    'feature2': np.random.randn(100),\n",
    "    'feature3': np.random.randn(100),\n",
    "    'target': np.random.randint(0, 2, 100)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = DataAnalyzer(df)\n",
    "analyzer.summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "analyzer.check_missing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, col in enumerate(['feature1', 'feature2', 'feature3'], 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.hist(df[col], bins=20, edgecolor='black')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = analyzer.prepare_features('target')\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    expr,\n",
    "    lower,\n",
    "    regexp_replace,\n",
    "    to_timestamp,\n",
    ")\n",
    "from pathlib import Path\n",
    "\n",
    "from src.common.config import RAW_DIR\n",
    "from src.de.spark_jobs.traffy_basic_etl import main\n",
    "\n",
    "RAW_FILE = Path(\"../data/raw/bangkok_traffy.csv\")\n",
    "OUTPUT_DIR = Path(\"../data/preprocessed/traffy_cleaned_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "sys.path.append(str(ROOT / \"src\"))\n",
    "os.environ[\"SPARK_LOCAL_DIRS\"] = str((ROOT / \"spark_tmp\").resolve())\n",
    "\n",
    "from src.de.spark_jobs import traffy_basic_etl as etl\n",
    "\n",
    "etl.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "out = Path(\"data/processed/traffy_cleaned_parquet\")\n",
    "if out.exists():\n",
    "    if out.is_file():\n",
    "        out.unlink()\n",
    "    else:\n",
    "        import shutil\n",
    "        shutil.rmtree(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Requirement already satisfied: scikit-learn in /home/sirav/spark_env/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/sirav/spark_env/lib/python3.10/site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/sirav/spark_env/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/sirav/spark_env/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/sirav/spark_env/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install findspark scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"C:\\Program Files\\Java\\jdk-21\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"C:\\spark\"\n",
    "# os.environ[\"HADOOP_HOME\"] = \"C:\\hadoop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_url = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/20 00:28:28 WARN Utils: Your hostname, sira, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/11/20 00:28:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/20 00:28:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.255.255.254:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Tutorial</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x766718e03400>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(spark_url)\\\n",
    "        .appName('Spark Tutorial')\\\n",
    "        .config('spark.ui.port', '4040')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sirav/JekTurnRight_dsde\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sirav/spark_env/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "üöÄ RUNNING TRAFFY FLOOD ETL PIPELINE\n",
      "============================================\n",
      "[STEP] Loading raw Traffy CSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Cleaning + schema validation + Bangkok filter...\n",
      "[STEP] Adding time columns...\n",
      "[STEP] Adding flood_flag column...\n",
      "[STEP] Aggregating daily flood counts per district...\n",
      "[STEP] Writing cleaned tickets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/20 00:31:00 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Writing daily flood time series...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "‚úÖ ETL COMPLETED\n",
      "   Cleaned data: /home/sirav/JekTurnRight_dsde/data/processed/traffy_clean.parquet\n",
      "   Flood TS: /home/sirav/JekTurnRight_dsde/data/processed/flood_daily_by_district.parquet\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "from src.de.spark_jobs.traffy_flood_etl import run_traffy_flood_etl\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "\n",
    "INPUT = f\"{ROOT}/data/raw/bangkok_traffy.csv\"\n",
    "CLEANED_OUT = f\"{ROOT}/data/processed/traffy_clean.parquet\"\n",
    "FLOOD_TS_OUT = f\"{ROOT}/data/processed/flood_daily_by_district.parquet\"\n",
    "\n",
    "run_traffy_flood_etl(\n",
    "    spark,\n",
    "    input_path=INPUT,\n",
    "    cleaned_output_path=CLEANED_OUT,\n",
    "    flood_ts_output_path=FLOOD_TS_OUT\n",
    ")\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "Requirement already satisfied: pandas in /home/sirav/spark_env/lib/python3.10/site-packages (2.3.3)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.7-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m294.9/294.9 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /home/sirav/spark_env/lib/python3.10/site-packages (1.7.2)\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 KB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tzdata>=2022.7 in /home/sirav/spark_env/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sirav/spark_env/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/sirav/spark_env/lib/python3.10/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sirav/spark_env/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.60.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting pyparsing>=3\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m113.9/113.9 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m325.0/325.0 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/sirav/spark_env/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting pillow>=8\n",
      "  Downloading pillow-12.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=3.1.0 in /home/sirav/spark_env/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/sirav/spark_env/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/sirav/spark_env/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m153.6/153.6 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m129.8/129.8 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.0/71.0 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m159.4/159.4 KB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/sirav/spark_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: urllib3, pyparsing, pillow, kiwisolver, idna, fonttools, cycler, contourpy, charset_normalizer, certifi, requests, matplotlib, seaborn\n",
      "Successfully installed certifi-2025.11.12 charset_normalizer-3.4.4 contourpy-1.3.2 cycler-0.12.1 fonttools-4.60.1 idna-3.11 kiwisolver-1.4.9 matplotlib-3.10.7 pillow-12.0.0 pyparsing-3.2.5 requests-2.32.5 seaborn-0.13.2 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas matplotlib seaborn scikit-learn requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+-----------+--------+-------------+---------+----+------------+--------------------+--------------------+----------+----+-----+---+----+-------+--------+\n",
      "|  ticket_id|         type|        organization|             comment|               photo|         photo_after|            coords|             address|subdistrict|district|     province|    state|star|count_reopen|       last_activity|           timestamp|      date|year|month|day|hour|weekday|is_flood|\n",
      "+-----------+-------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+-----------+--------+-------------+---------+----+------------+--------------------+--------------------+----------+----+-----+---+----+-------+--------+\n",
      "|2021-4D9Y98|           {}|‡πÄ‡∏Ç‡∏ï‡∏•‡∏≤‡∏î‡∏û‡∏£‡πâ‡∏≤‡∏ß,‡∏Å‡∏≤‡∏£‡πÑ‡∏ü...|‡∏´‡∏ô‡πâ‡∏≤‡∏õ‡∏≤‡∏Å‡∏ã‡∏≠‡∏¢ ‡∏•‡∏≤‡∏î‡∏û‡∏£‡πâ...|https://storage.g...|https://storage.g...|100.59131,13.80910|17/73 17/73 ‡∏ñ. ‡∏•‡∏≤...|   ‡∏•‡∏≤‡∏î‡∏û‡∏£‡πâ‡∏≤‡∏ß|‡∏•‡∏≤‡∏î‡∏û‡∏£‡πâ‡∏≤‡∏ß|‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£|‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô|NULL|           0|2023-03-14 12:09:...|2021-12-13 12:53:...|2021-12-13|2021|   12| 13|  12|      2|   false|\n",
      "|2021-7K6QA3|           {}|‡πÄ‡∏Ç‡∏ï‡∏õ‡∏£‡∏∞‡πÄ‡∏ß‡∏®,‡∏ù‡πà‡∏≤‡∏¢‡πÄ‡∏ó‡∏®...|‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡∏•‡∏±‡∏á‡πÜ ‡∏ô‡∏µ‡πâ ‡∏û‡∏ö‡πÄ...|https://storage.g...|https://storage.g...|100.65617,13.72812|208/22 ‡∏ñ. ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏Å‡∏≤...|     ‡∏õ‡∏£‡∏∞‡πÄ‡∏ß‡∏®|  ‡∏õ‡∏£‡∏∞‡πÄ‡∏ß‡∏®|‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£|‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô|   2|           0|2022-06-24 06:32:...|2021-12-22 06:03:...|2021-12-22|2021|   12| 22|   6|      4|   false|\n",
      "|2021-7U9RED|           {}|            ‡πÄ‡∏Ç‡∏ï‡∏î‡∏∏‡∏™‡∏¥‡∏ï|‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡πÑ...|https://storage.g...|https://storage.g...|100.50848,13.77832|627 ‡∏ñ‡∏ô‡∏ô‡∏™‡∏≤‡∏°‡πÄ‡∏™‡∏ô ‡πÅ‡∏Ç‡∏ß...|      ‡∏î‡∏∏‡∏™‡∏¥‡∏ï|   ‡∏î‡∏∏‡∏™‡∏¥‡∏ï|‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£|‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô|   5|           0|2023-05-17 06:11:...|2021-12-17 15:46:...|2021-12-17|2021|   12| 17|  15|      6|   false|\n",
      "|2021-8BTWZB|{‡∏ó‡πà‡∏≠‡∏£‡∏∞‡∏ö‡∏≤‡∏¢‡∏ô‡πâ‡∏≥}|‡πÄ‡∏Ç‡∏ï‡∏õ‡∏£‡∏∞‡πÄ‡∏ß‡∏®,‡∏ù‡πà‡∏≤‡∏¢‡πÇ‡∏¢‡∏ò...|‡∏Ç‡∏≠‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡πà‡∏≠‡∏£‡∏∞...|https://storage.g...|https://storage.g...|100.65440,13.68158|70 ‡∏ã‡∏≠‡∏¢ ‡πÄ‡∏â‡∏•‡∏¥‡∏°‡∏û‡∏£‡∏∞‡πÄ‡∏Å...|    ‡∏´‡∏ô‡∏≠‡∏á‡∏ö‡∏≠‡∏ô|  ‡∏õ‡∏£‡∏∞‡πÄ‡∏ß‡∏®|‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£|‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô|   5|           0|2022-06-20 13:12:...|2021-12-22 17:15:...|2021-12-22|2021|   12| 22|  17|      4|    true|\n",
      "|2021-8N9ZP8|  {‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î}|‡πÄ‡∏Ç‡∏ï‡∏õ‡∏£‡∏∞‡πÄ‡∏ß‡∏®,‡∏ù‡πà‡∏≤‡∏¢‡πÄ‡∏ó‡∏®...|‡∏Ñ‡∏ô‡πÄ‡∏≠‡∏≤‡∏Ç‡∏¢‡∏∞‡∏°‡∏≤‡∏ó‡∏¥‡πâ‡∏á‡∏à‡∏ô‡∏Å...|https://storage.g...|https://storage.g...|100.64690,13.67083|110 ‡∏ã‡∏≠‡∏¢ ‡∏°‡∏µ‡∏™‡∏∏‡∏Ç ‡πÅ‡∏Ç‡∏ß...|    ‡∏´‡∏ô‡∏≠‡∏á‡∏ö‡∏≠‡∏ô|  ‡∏õ‡∏£‡∏∞‡πÄ‡∏ß‡∏®|‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£|‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô|   2|           0|2024-11-26 04:17:...|2021-12-18 21:50:...|2021-12-18|2021|   12| 18|  21|      7|   false|\n",
      "+-----------+-------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+-----------+--------+-------------+---------+----+------------+--------------------+--------------------+----------+----+-----+---+----+-------+--------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- ticket_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- organization: string (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      " |-- photo: string (nullable = true)\n",
      " |-- photo_after: string (nullable = true)\n",
      " |-- coords: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- subdistrict: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- star: string (nullable = true)\n",
      " |-- count_reopen: string (nullable = true)\n",
      " |-- last_activity: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- is_flood: boolean (nullable = true)\n",
      "\n",
      "+-----------+\n",
      "|   district|\n",
      "+-----------+\n",
      "|   ‡∏ï‡∏•‡∏¥‡πà‡∏á‡∏ä‡∏±‡∏ô|\n",
      "|   ‡∏î‡∏≠‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á|\n",
      "|     ‡∏ö‡∏≤‡∏á‡∏£‡∏±‡∏Å|\n",
      "|     ‡∏õ‡∏£‡∏∞‡πÄ‡∏ß‡∏®|\n",
      "|     ‡∏ö‡∏≤‡∏á‡∏ö‡∏≠‡∏ô|\n",
      "|      ‡∏î‡∏∏‡∏™‡∏¥‡∏ï|\n",
      "|      ‡∏û‡∏ç‡∏≤‡πÑ‡∏ó|\n",
      "|    ‡∏´‡∏ô‡∏≠‡∏á‡∏à‡∏≠‡∏Å|\n",
      "|    ‡∏£‡∏≤‡∏ä‡πÄ‡∏ó‡∏ß‡∏µ|\n",
      "|‡∏ö‡∏≤‡∏á‡∏Ç‡∏∏‡∏ô‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô|\n",
      "|      ‡∏ö‡∏≤‡∏á‡πÅ‡∏Ñ|\n",
      "|‡∏£‡∏≤‡∏©‡∏é‡∏£‡πå‡∏ö‡∏π‡∏£‡∏ì‡∏∞|\n",
      "|    ‡∏Ñ‡∏•‡∏≠‡∏á‡πÄ‡∏ï‡∏¢|\n",
      "|     ‡∏à‡∏≠‡∏°‡∏ó‡∏≠‡∏á|\n",
      "|  ‡∏Ñ‡∏•‡∏≠‡∏á‡∏™‡∏≤‡∏°‡∏ß‡∏≤|\n",
      "| ‡∏ö‡∏≤‡∏á‡∏Å‡∏≠‡∏Å‡πÉ‡∏´‡∏ç‡πà|\n",
      "|    ‡∏ó‡∏∏‡πà‡∏á‡∏Ñ‡∏£‡∏∏|\n",
      "|   ‡∏•‡∏≤‡∏î‡∏û‡∏£‡πâ‡∏≤‡∏ß|\n",
      "|     ‡∏ö‡∏≤‡∏á‡πÄ‡∏Ç‡∏ô|\n",
      "|   ‡∏ó‡∏ß‡∏µ‡∏ß‡∏±‡∏í‡∏ô‡∏≤|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "+-----------+-----+\n",
      "|   district|count|\n",
      "+-----------+-----+\n",
      "|    ‡∏à‡∏ï‡∏∏‡∏à‡∏±‡∏Å‡∏£| 1252|\n",
      "|     ‡∏ö‡∏≤‡∏á‡πÄ‡∏Ç‡∏ô|  571|\n",
      "|    ‡∏£‡∏≤‡∏ä‡πÄ‡∏ó‡∏ß‡∏µ|  556|\n",
      "|    ‡∏™‡∏ß‡∏ô‡∏´‡∏•‡∏ß‡∏á|  535|\n",
      "|     ‡∏õ‡∏£‡∏∞‡πÄ‡∏ß‡∏®|  522|\n",
      "|   ‡∏´‡πâ‡∏ß‡∏¢‡∏Ç‡∏ß‡∏≤‡∏á|  503|\n",
      "|    ‡∏ö‡∏≤‡∏á‡∏ã‡∏∑‡πà‡∏≠|  478|\n",
      "|      ‡∏ß‡∏±‡∏í‡∏ô‡∏≤|  468|\n",
      "|    ‡∏Ñ‡∏•‡∏≠‡∏á‡πÄ‡∏ï‡∏¢|  444|\n",
      "|    ‡∏ö‡∏≤‡∏á‡∏Å‡∏∞‡∏õ‡∏¥|  442|\n",
      "|      ‡∏ö‡∏≤‡∏á‡πÅ‡∏Ñ|  438|\n",
      "|      ‡∏û‡∏ç‡∏≤‡πÑ‡∏ó|  425|\n",
      "|    ‡∏õ‡∏ó‡∏∏‡∏°‡∏ß‡∏±‡∏ô|  425|\n",
      "| ‡∏ö‡∏≤‡∏á‡∏Å‡∏≠‡∏Å‡∏ô‡πâ‡∏≠‡∏¢|  420|\n",
      "|‡∏ö‡∏≤‡∏á‡∏Ç‡∏∏‡∏ô‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô|  409|\n",
      "| ‡∏ß‡∏±‡∏á‡∏ó‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏á|  394|\n",
      "|    ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏µ‡πà|  392|\n",
      "|  ‡∏Ñ‡∏•‡∏≠‡∏á‡∏™‡∏≤‡∏°‡∏ß‡∏≤|  389|\n",
      "|    ‡∏ö‡∏≤‡∏á‡∏û‡∏•‡∏±‡∏î|  386|\n",
      "|     ‡∏ö‡∏≤‡∏á‡∏£‡∏±‡∏Å|  381|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "+-----------+---------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+-----------+--------+-------------+---------+----+------------+--------------------+--------------------+----------+----+-----+---+----+-------+--------+\n",
      "|  ticket_id|     type|        organization|             comment|               photo|         photo_after|            coords|             address|subdistrict|district|     province|    state|star|count_reopen|       last_activity|           timestamp|      date|year|month|day|hour|weekday|is_flood|\n",
      "+-----------+---------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+-----------+--------+-------------+---------+----+------------+--------------------+--------------------+----------+----+-----+---+----+-------+--------+\n",
      "|2022-36DMWD|       {}|‡πÄ‡∏Ç‡∏ï‡∏ö‡∏≤‡∏á‡∏£‡∏±‡∏Å,‡∏ù‡πà‡∏≤‡∏¢‡πÇ‡∏¢‡∏ò...|‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ã‡πà‡∏≠‡∏°‡∏ó‡∏≤‡∏á...|https://storage.g...|https://storage.g...|100.51525,13.72327|38 40 ‡∏ñ. ‡πÄ‡∏à‡∏£‡∏¥‡∏ç‡∏Å‡∏£‡∏∏...|     ‡∏ö‡∏≤‡∏á‡∏£‡∏±‡∏Å|  ‡∏ö‡∏≤‡∏á‡∏£‡∏±‡∏Å|‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£|‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô|   5|           0|2023-02-07 03:49:...|2022-06-10 12:43:...|2022-06-10|2022|    6| 10|  12|      6|   false|\n",
      "|2022-3AQ26L|    {‡∏ñ‡∏ô‡∏ô}|‡πÄ‡∏Ç‡∏ï‡∏ö‡∏≤‡∏á‡∏£‡∏±‡∏Å,‡∏ú‡∏≠.‡πÄ‡∏Ç‡∏ï‡∏ö...|‡πÉ‡∏ô‡∏ñ‡∏ô‡∏ô‡∏®‡∏≤‡∏•‡∏≤‡πÅ‡∏î‡∏á 1  ‡πÄ...|https://storage.g...|                NULL|100.53736,13.72541|124 ‡∏ñ‡∏ô‡∏ô‡∏®‡∏≤‡∏•‡∏≤‡πÅ‡∏î‡∏á ‡πÅ‡∏Ç...|       ‡∏™‡∏µ‡∏•‡∏°|  ‡∏ö‡∏≤‡∏á‡∏£‡∏±‡∏Å|‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£|‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô|NULL|           0|2023-02-07 02:51:...|2022-06-05 16:44:...|2022-06-05|2022|    6|  5|  16|      1|   false|\n",
      "|2022-3B9AUM|       {}|‡πÄ‡∏Ç‡∏ï‡∏ö‡∏≤‡∏á‡∏£‡∏±‡∏Å,‡∏ú‡∏≠.‡πÄ‡∏Ç‡∏ï‡∏ö...|    ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏á|https://storage.g...|                NULL|100.53378,13.73055|942/8-9 ‡∏ñ‡∏ô‡∏ô‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏°...|  ‡∏™‡∏∏‡∏£‡∏¥‡∏¢‡∏ß‡∏á‡∏®‡πå|  ‡∏ö‡∏≤‡∏á‡∏£‡∏±‡∏Å|‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£|‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô|NULL|           0|2023-02-07 03:19:...|2022-06-08 09:31:...|2022-06-08|2022|    6|  8|   9|      4|   false|\n",
      "|2022-3CCB8E|       {}|‡πÄ‡∏Ç‡∏ï‡∏ö‡∏≤‡∏á‡∏£‡∏±‡∏Å,‡∏ú‡∏≠.‡πÄ‡∏Ç‡∏ï‡∏ö...|       ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡πà‡∏∞|https://storage.g...|                NULL|100.52448,13.73429|596/7 ‡∏ñ‡∏ô‡∏ô‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏°‡∏ó‡∏µ...| ‡∏°‡∏´‡∏≤‡∏û‡∏§‡∏í‡∏≤‡∏£‡∏≤‡∏°|  ‡∏ö‡∏≤‡∏á‡∏£‡∏±‡∏Å|‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£|‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô|   5|           0|2023-02-07 04:13:...|2022-06-10 20:34:...|2022-06-10|2022|    6| 10|  20|      6|   false|\n",
      "|2022-3EEWQ7|{‡∏Å‡∏µ‡∏î‡∏Ç‡∏ß‡∏≤‡∏á}|‡πÄ‡∏Ç‡∏ï‡∏ö‡∏≤‡∏á‡∏£‡∏±‡∏Å,‡∏™‡∏ô.‡∏ó‡∏∏‡πà‡∏á...|‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡∏à‡∏≠‡∏î‡∏Å‡∏µ‡∏î‡∏Ç‡∏ß‡∏≤‡∏á‡∏Å...|https://storage.g...|                NULL|100.53134,13.72368|44 4 ‡∏ñ‡∏ô‡∏ô ‡∏™‡∏µ‡∏•‡∏° ‡πÅ‡∏Ç‡∏ß...|       ‡∏™‡∏µ‡∏•‡∏°|  ‡∏ö‡∏≤‡∏á‡∏£‡∏±‡∏Å|‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£|‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô|NULL|           0|2023-02-07 03:10:...|2022-06-08 07:25:...|2022-06-08|2022|    6|  8|   7|      4|   false|\n",
      "+-----------+---------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+-----------+--------+-------------+---------+----+------------+--------------------+--------------------+----------+----+-----+---+----+-------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# use spark read this /home/sirav/JekTurnRight_dsde/data/processed/traffy_clean.parquet\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JekTurnRight Analysis\") \\\n",
    "    .getOrCreate()\n",
    "df = spark.read.parquet(\"/home/sirav/JekTurnRight_dsde/data/processed/traffy_clean.parquet\")\n",
    "df.show(5)\n",
    "\n",
    "df.printSchema()\n",
    "df.count()\n",
    "df.select(\"district\").distinct().show()\n",
    "df.groupBy(\"district\").count().orderBy(\"count\", ascending=False).show()\n",
    "df.filter(df[\"district\"] == \"‡∏ö‡∏≤‡∏á‡∏£‡∏±‡∏Å\").show(5)\n",
    "df.createOrReplaceTempView(\"traffy_data\")\n",
    "# result = spark.sql(\"SELECT district, COUNT(*) as report_count FROM traffy_data GROUP BY\n",
    "#     district ORDER BY report_count DESC\")\n",
    "# result.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------------------+\n",
      "|      date|   district|flood_complaint_count|\n",
      "+----------+-----------+---------------------+\n",
      "|2021-09-19|     ‡∏õ‡∏£‡∏∞‡πÄ‡∏ß‡∏®|                    1|\n",
      "|2021-12-22|     ‡∏õ‡∏£‡∏∞‡πÄ‡∏ß‡∏®|                    1|\n",
      "|2022-01-14|       ‡∏™‡∏≤‡∏ó‡∏£|                    1|\n",
      "|2022-02-04|       ‡∏™‡∏≤‡∏ó‡∏£|                    1|\n",
      "|2022-02-26|      ‡∏î‡∏∏‡∏™‡∏¥‡∏ï|                    1|\n",
      "|2022-03-03|   ‡∏ï‡∏•‡∏¥‡πà‡∏á‡∏ä‡∏±‡∏ô|                    1|\n",
      "|2022-04-01|      ‡∏ö‡∏≤‡∏á‡∏ô‡∏≤|                    1|\n",
      "|2022-05-18|   ‡∏•‡∏≤‡∏î‡∏û‡∏£‡πâ‡∏≤‡∏ß|                    1|\n",
      "|2022-05-20|   ‡∏•‡∏≤‡∏î‡∏û‡∏£‡πâ‡∏≤‡∏ß|                    1|\n",
      "|2022-05-23|     ‡∏ö‡∏≤‡∏á‡πÄ‡∏Ç‡∏ô|                    1|\n",
      "|2022-05-24|   ‡∏•‡∏≤‡∏î‡∏û‡∏£‡πâ‡∏≤‡∏ß|                    1|\n",
      "|2022-05-25|   ‡∏•‡∏≤‡∏î‡∏û‡∏£‡πâ‡∏≤‡∏ß|                    3|\n",
      "|2022-05-26|    ‡∏à‡∏ï‡∏∏‡∏à‡∏±‡∏Å‡∏£|                    1|\n",
      "|2022-05-26|    ‡∏ö‡∏∂‡∏á‡∏Å‡∏∏‡πà‡∏°|                    1|\n",
      "|2022-05-27|   ‡∏´‡πâ‡∏ß‡∏¢‡∏Ç‡∏ß‡∏≤‡∏á|                    1|\n",
      "|2022-05-28|‡∏ö‡∏≤‡∏á‡∏Ç‡∏∏‡∏ô‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô|                    1|\n",
      "|2022-05-28| ‡∏ß‡∏±‡∏á‡∏ó‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏á|                    1|\n",
      "|2022-05-28|    ‡∏£‡∏≤‡∏ä‡πÄ‡∏ó‡∏ß‡∏µ|                    1|\n",
      "|2022-05-28|     ‡∏ö‡∏≤‡∏á‡∏ö‡∏≠‡∏ô|                    1|\n",
      "|2022-05-29|    ‡∏¢‡∏≤‡∏ô‡∏ô‡∏≤‡∏ß‡∏≤|                    1|\n",
      "+----------+-----------+---------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# read /home/sirav/JekTurnRight_dsde/data/processed/flood_daily_by_district.parquet\n",
    "flood_df = spark.read.parquet(\"/home/sirav/JekTurnRight_dsde/data/processed/flood_daily_by_district.parquet\")\n",
    "# flood_df.show()\n",
    "\n",
    "# sort by date\n",
    "flood_df = flood_df.orderBy(\"date\")\n",
    "flood_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PROJECT_ROOT, RAW_DIR\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFILE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROJECT_ROOT:\u001b[39m\u001b[38;5;124m\"\u001b[39m, PROJECT_ROOT)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAW_DIR:\u001b[39m\u001b[38;5;124m\"\u001b[39m, RAW_DIR)\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from src.common.config import PROJECT_ROOT, RAW_DIR\n",
    "print(\"FILE:\", Path(__file__).resolve())\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"RAW_DIR:\", RAW_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
